# -*- coding: utf-8 -*-
"""G5 AiP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AEGxmbhSG9JjAoZTgw2_HKP0NsoxtzVA

# 1. Introduction

## 1.1 Import Essential Libraries
"""

import pandas as pd
import numpy as np

"""## 1.2 Import Datasets

"""

df_reviews = pd.read_csv("olist_order_reviews_dataset.csv")
df_order_items = pd.read_csv("olist_order_items_dataset.csv")
df_orders = pd.read_csv("olist_orders_dataset.csv")
df_products = pd.read_csv("olist_products_dataset.csv")
df_customers = pd.read_csv("olist_customers_dataset.csv")
df_payments = pd.read_csv("olist_order_payments_dataset.csv")
df_product_translations = pd.read_csv("product_category_name_translation.csv")

"""# 2. Data Cleansing

## 2.1 Customers Dataset

1) Drop Columns
"""

df_customers.head()
df_customers = df_customers.drop(['customer_zip_code_prefix', 'customer_city'], axis=1)

"""2) Check for null values"""

null_values = df_customers.isnull().sum()
print(null_values)

"""3) Check for duplicates"""

duplicates = df_customers['customer_id'].duplicated().sum()
print(f"Number of duplicate customer_id values: {duplicates}")

"""## 2.2 Orders Dataset"""

df_orders.head()

"""1) Check for unreasonable data."""

# List of columns to convert
date_columns = ['order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']

# Apply pd.to_datetime to each column
df_orders[date_columns] = df_orders[date_columns].apply(pd.to_datetime)

# Find rows where carrier date is later than delivery date
df_orders['time_to_delivery'] = df_orders['order_delivered_customer_date'] - df_orders['order_delivered_carrier_date']

# delete all the canceled orders
df_orders = df_orders[df_orders['order_status'] != 'canceled']

# count the each label in order_status after dropping canceled values
status_counts = df_orders['order_status'].value_counts()
print(status_counts)

# Count rows where time_to_delivery is negative
negative_count = (df_orders['time_to_delivery'] < pd.Timedelta(0)).sum()

# Print the count of negative values
print(f"Number of negative time_to_delivery values: {negative_count}")

# Remove rows with negative time_to_delivery
df_orders = df_orders[df_orders['time_to_delivery'] >= pd.Timedelta(0)]

# Check the result by counting again
negative_count_after = (df_orders['time_to_delivery'] < pd.Timedelta(0)).sum()
print(f"Number of negative time_to_delivery values after deletion: {negative_count_after}")

# Find duplicates based on specific columns
duplicates_by_columns = df_orders[df_orders.duplicated(subset=['order_id'])]

# Print duplicate rows based on selected columns
print(duplicates_by_columns)

df_orders = df_orders.drop(['order_purchase_timestamp', 'order_approved_at','time_to_delivery','order_status'], axis=1)
df_orders.head()

"""## 2.3 Reviews Dataset

1) Check for null values
"""

df_reviews.isnull().sum()

"""2) Duplicate check for review_id column"""

df_reviews['review_id'].duplicated().sum()
df_reviews = df_reviews.drop_duplicates(subset='review_id', keep='first')

"""3)Drop unwanted columns"""

df_reviews = df_reviews.drop(['review_id', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp'], axis=1)
df_reviews

"""## 2.4 Products Dataset

1) Merging products translation table with producst table
"""

products_translated_df = pd.merge(df_products, df_product_translations, on="product_category_name", how="left")

"""2) Dropping unecessary columns"""

products_translated_df= products_translated_df.drop(['product_category_name', 'product_name_lenght'], axis=1)

"""3) Replacing product categories with errors
*   _2 character at the end of the name
*   home_confort insted of comfort


"""

products_translated_df['product_category_name_english'] = products_translated_df['product_category_name_english'].str.replace(r'_2$', '', regex=True)
products_translated_df['product_category_name_english'] = products_translated_df['product_category_name_english'].str.replace(r'home_confort', 'home_comfort', regex=True)

"""## 2.5 Order Items Dataset

1) Checking for Duplicate Order id see how many we have
"""

df_order_items[df_order_items.duplicated(['order_id'], keep=False)]
df_order_items.drop_duplicates(subset=['order_id'], inplace=True)

"""2)Checking for negative values in price and freight value"""

print(df_order_items[df_order_items['price'] < 0].count())
print(df_order_items[df_order_items['freight_value'] < 0].count())

"""3)Dropping Unecessary Columns"""

df_order_items=df_order_items.drop(['shipping_limit_date','order_item_id'], axis=1)

"""## 2.6 Payments Dataset"""

df_payments = df_payments.drop(['payment_sequential', 'payment_installments'], axis=1)

df_payments = df_payments[df_payments['payment_type'] != 'not defined']

"""# 3. Merging Tables"""

# Merge reviews with orders
final_df = pd.merge(df_reviews, df_orders, on='order_id', how='inner')

# Merge order items and products with orders
final_df = pd.merge(final_df, df_order_items, on='order_id', how='inner')
final_df = pd.merge(final_df, df_payments, on='order_id', how='inner')
final_df = pd.merge(final_df, products_translated_df, on='product_id', how='inner')

# Merge customers with orders
final_df = pd.merge(final_df, df_customers, on='customer_id', how='inner')

final_df

"""**Summary Table**"""

#Removing Null Values
final_df = final_df.dropna()
final_df = final_df.copy()

summary_table = pd.DataFrame({
    'Data Type': final_df.dtypes,
    'Unique Values': final_df.nunique(),
    'Missing Values': final_df.isnull().sum(),
    'Count': final_df.count(),
    'Standard Deviation': final_df.std(numeric_only=True),
    'Median': final_df.median(numeric_only=True),
    'Mean': final_df.mean(numeric_only=True),
    'Min': final_df.min(numeric_only=True),
    'Max': final_df.max(numeric_only=True)
})

# Display the customized summary table
summary_table

"""# 4. Feature Engineering

## 4.1 Adding Feature

1) Product Volume
"""

final_df['product_volume'] = final_df['product_length_cm'] * final_df['product_height_cm'] * final_df['product_width_cm']

"""2) Delivery Duration"""

final_df['delivery_duration'] = (final_df['order_delivered_customer_date'] - final_df['order_delivered_carrier_date']).dt.days

"""3) Delivery on time"""

final_df['delivery_on_time'] = (final_df['order_delivered_customer_date'] <= final_df['order_estimated_delivery_date']).astype(int)

"""## 4.2 Remove Features"""

print(final_df.columns)

"""1) Drop Unecessary columns"""

final_df = final_df.drop(["customer_id",
    "customer_unique_id",
    "order_id",
    "product_id",
    "seller_id",
    'product_length_cm',
    'product_height_cm',
    'product_width_cm',
    "freight_value",
    "payment_value",
    "order_delivered_customer_date",
    "order_delivered_carrier_date",
    "order_estimated_delivery_date"], axis=1)

"""2) Merging product categories"""

# Load the product dataset and category mapping
category_mapping = pd.read_excel('prdct_category.xlsx')

category_mapping_dict = dict(zip(category_mapping['product_category_name_english'], category_mapping['merged_category']))
final_df['merged_category'] = final_df['product_category_name_english'].map(category_mapping_dict)

# Fill missing values with a default category
final_df['merged_category'].fillna('Other', inplace=True)

print(final_df.columns)
final_df

"""Download Database"""

from google.colab import files

# Save the DataFrame to a CSV file
file_name = "final_df.csv"
final_df.to_csv(file_name, index=False)

# Download the file
files.download(file_name)

"""# 5. Explanatory Data Analysis

###5.1 Descriptive Analysis
"""

final_df.head()

final_df.info()

final_df.isnull().sum()

final_df.describe()

"""###5.2 Graphical Ananlysis

Importing Libraries for visualization
"""

import matplotlib.pyplot as plt
import seaborn as sns

"""Visualization 1: Review Score Distribution"""

# Calculate the total count for percentages
total_count = final_df['review_score'].count()

# Plot the bar chart
plt.figure(figsize=(8, 6))
ax = sns.countplot(data=final_df, x="review_score", palette="viridis")

# Add count and percentage labels on the bars
for bar in ax.patches:
    count = int(bar.get_height())  # Get the height of each bar
    percentage = (count / total_count) * 100  # Calculate the percentage
    ax.text(
        bar.get_x() + bar.get_width() / 2,  # X-coordinate (center of bar)
        count + 0.02 * max([bar.get_height() for bar in ax.patches]),  # Y-coordinate slightly above bar
        f'{count:,} ({percentage:.1f}%)',  # Add count and percentage
        ha='center', va='bottom', fontsize=8, color='black'
    )

# Add title and labels
plt.title('Distribution of Review Scores', fontsize=16)
plt.xlabel('Review Score', fontsize=14)
plt.ylabel('Count', fontsize=14)

# Adjust layout and show the plot
plt.tight_layout()
plt.show()

"""Explaination: The graph shows a distribution of review scores, with a clear preference for higher ratings. The majority of reviews are 4 or 5 stars, indicating overall positive sentiment.

Visualization 2: Product Price Distribution
"""

# Product Price Distribution (Log Scale)
plt.figure(figsize=(8, 5))
sns.histplot(final_df['price'], kde=True, bins=50, color='skyblue', log_scale=True)
plt.title("Distribution of Product Prices", fontsize=14)
plt.xlabel("Price (Log Scale)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

"""Explanation:
* The majority of products are priced between \$10 and \$100, as indicated by the highest bars in this range. The peak frequency is around 8,000 products.
* The distribution is skewed, with fewer products priced below \$10 and above \$1000. Highend products priced above \$1,000 are relatively rare.
* The x-axis uses a logarithmic scale to represent a wide price range (from \$1 to \$10,000), allowing clearer visualization of trends and reducing the dominance of outliers.

Visualization 3: Review Score Distribution by Delivery Status
"""

# Create a stacked bar chart
stacked_data = final_df.groupby(['delivery_on_time', 'review_score']).size().unstack(fill_value=0)

# Plot
stacked_data.plot(kind='bar', stacked=True, figsize=(6, 4), colormap='Blues')
plt.title('Review Score Distribution by Delivery Status')
plt.xlabel('Delivery Status (0 = Late, 1 = On Time)')
plt.ylabel('Number of Deliveries')
plt.legend(title='Review Score', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""Explanation:
* Deliveries that were late (0) are fewer in number compared to on-time deliveries (1).
* Among the on-time deliveries, there is a significant number of 5 (yellow) and 4 (orange) review scores, suggesting that on-time delivery is strongly associated with higher review scores.
* Late deliveries also have 5 and 4 ratings, but in much smaller proportions, and there are noticeable lower scores (1, 2, and 3).

Visualization 4: Correlation between Delivery time and Review Score
"""

# Assuming 'final_df' contains the relevant columns
correlation_matrix = final_df[['review_score', 'delivery_duration']].corr()

# Plotting the heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt='.2f', cbar=True)

# Adding title
plt.title('Correlation Heatmap between Review Score and Delivery Duration')

plt.show()

"""Explanation:
* The heatmap visualizes the correlation between two variables: review_score and delivery_duration.
* The diagonal values show the correlation of each variable with itself, which is always 1.00. This is why the top-left (review_score vs. review_score) and bottom-right (delivery_duration vs. delivery_duration) cells are dark blue, representing a perfect correlation.
* The correlation between review_score and delivery_duration is -0.30, indicating a weak negative correlation. This means as delivery_duration increases, review_score tends to decrease, but the relationship is not strong.
* The negative correlation suggests that faster deliveries (shorter delivery durations) might be associated with higher review scores, but the correlation isn’t particularly strong

Visualization 5: Box Plot of Time to Delivery
"""

# Create a box plot for time_to_delivery
plt.figure(figsize=(6, 4))
sns.boxplot(x=final_df['delivery_duration'], color='skyblue')

# Adding labels and title
plt.title('Box Plot of Time to Delivery')
plt.xlabel('Time to Delivery (Days)')

# Display the plot
plt.tight_layout()
plt.show()

# Example: Scatter plot for price vs review_score
plt.figure(figsize=(8, 6))
sns.scatterplot(x=final_df['price'], y=final_df['review_score'], color="purple", alpha=0.7)
plt.title("Scatter Plot Showing Outliers", fontsize=14)
plt.xlabel("Price", fontsize=12)
plt.ylabel("Review Score", fontsize=12)
plt.show()

"""Explanation:
* The bulk of delivery durations is close to 0–10 days, as shown by the compact box and whiskers.
* There are a significant number of delivery durations that exceed the upper limit. Many of these outliers extend up to 200 days, which indicates a small subset of extremely long delivery times.

Visualization 6: Product Description Length vs Review Score (Scatter Plot)
"""

# Scatter plot with filtered data
plt.figure(figsize=(6, 4))
sns.scatterplot(
    data=final_df,
    x='product_description_lenght',
    y='review_score',
    alpha=0.7,
    hue='review_score',
    palette='viridis'
)

# Add labels and title
plt.title('Scatter Plot: Product Description Length vs Review Score')
plt.xlabel('Product Description Length')
plt.ylabel('Review Score')

# Set y-axis ticks to integer values
plt.yticks([1, 2, 3, 4, 5])

# Move the legend outside the plot
plt.legend(title='Review Score', loc='upper left', bbox_to_anchor=(1, 1))

# Adjust layout to fit the legend outside
plt.tight_layout()

# Show the plot
plt.show()

"""Explanation:
* For all review scores, the description length varies significantly (from short to long descriptions).
* This suggests no clear trend where longer descriptions directly correspond to higher or lower scores.
* The scatter plot doesn't show a strong correlation between the two variables.

Visualization 7: Proportion of Payment Types (Donut Chart)
"""

# Assuming 'payment_type' is a column in the DataFrame
payment_counts = final_df['payment_type'].value_counts()

# Calculate total count
total_count = payment_counts.sum()

# Create labels with both count and percentage
labels = [f'{label.replace("_", " ").title()}: {count} ({(count / total_count) * 100:.1f}%)'
          for label, count in zip(payment_counts.index, payment_counts)]

# Plotting the donut chart
plt.figure(figsize=(5, 5))
plt.pie(payment_counts, labels=labels, startangle=140, colors=sns.color_palette('viridis', len(payment_counts)), wedgeprops={'width': 0.4})

# Add a circle at the center to make it a donut chart
center_circle = plt.Circle((0, 0), 0.70, fc='white')
plt.gca().add_artist(center_circle)

# Title of the chart
plt.title('Proportion of Payment Types (Donut Chart)')

# Show the plot
plt.show()

"""Explanation:
This is a donut chart (a variation of a pie chart) that shows the proportion of different payment types. The chart visually represents the percentage distribution of four payment types: credit_card, debit_card, voucher, and boleto.

Visualization 8: Count of 4 and 5 Ratings by Customer State
"""

# Filter the dataset for review scores 4 and 5
filtered_df = final_df[final_df['review_score'].isin([4, 5])]

# Group by customer_state and count the occurrences of 4 and 5 ratings
rating_count_by_state = filtered_df.groupby('customer_state')['review_score'].count()

# Sort the states by the count of 4 and 5 ratings in descending order
rating_count_by_state = rating_count_by_state.sort_values(ascending=False)

# Display the state with the maximum count of 4 and 5 ratings
max_state = rating_count_by_state.idxmax()
max_count = rating_count_by_state.max()

print(f"State with maximum 4 and 5 ratings: {max_state}")
print(f"Count of 4 and 5 ratings: {max_count}")

# Plot the count of 4 and 5 ratings by state
plt.figure(figsize=(12, 6))
sns.barplot(x=rating_count_by_state.index, y=rating_count_by_state.values, palette='plasma')
plt.xticks(rotation=90)
plt.title('Count of 4 and 5 Ratings by Customer State')
plt.xlabel('Customer State')
plt.ylabel('Count of 4 and 5 Ratings')
plt.show()

"""Explanation:
* The state "SP" (São Paulo) has the highest number of 4 and 5 ratings by a significant margin, with over 30,000 reviews. This suggests that SP has the largest customer base or a higher tendency for positive reviews.
* There is a clear skew, with a few states (e.g., SP, RJ, MG) accounting for the majority of high ratings, while many states contribute minimally.

Visualization 9: Average Review Score by Product Category
"""

# Group by product_category_name_english and calculate the mean review_score
category_avg_rating = final_df.groupby('merged_category')['review_score'].mean()

# Sort the results to get the category with the highest average rating
category_avg_rating = category_avg_rating.sort_values(ascending=False)

# Display the category with the highest average rating
top_category = category_avg_rating.idxmax()
top_category_rating = category_avg_rating.max()

print(f"Product Category with the highest average review score: {top_category}")
print(f"Average Review Score: {top_category_rating}")


# Plot the average review score by product category using a line plot
plt.figure(figsize=(8, 4))
sns.lineplot(x=category_avg_rating.index, y=category_avg_rating.values, marker='o', palette='viridis')
plt.xticks(rotation=45)
plt.title('Average Review Score by Product Category (Line Plot)')
plt.xlabel('Product Category')
plt.ylabel('Average Review Score')
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select numeric columns for correlation
numeric_columns = final_df.select_dtypes(include=['int64', 'float64'])

# Calculate the correlation matrix
correlation_matrix = numeric_columns.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)

# Add title and adjust layout
plt.title('Correlation Heatmap of Variables', fontsize=16, fontweight='bold')
plt.tight_layout()

# Show the heatmap
plt.show()

"""Explanation:
* Categories closer to the left (e.g., "Books") perform better on average, while categories toward the right (e.g., "Furniture") tend to have lower scores.
*  The steady decline suggests that satisfaction levels decrease for certain product types, possibly due to complexity, expectations, or usability.

# 6. Data Processing
"""

model_df = final_df
model_df

"""## 6.1 Define the binary target

Convert the review_score into a binary variable:
1 for "good" (scores 5 and 4).
0 for "not good" (scores 3, 2, and 1).
"""

# Convert the 'review_score' into a binary variable using np.where:
# 1 for "good" (scores 5 and 4).
# 0 for "not good" (scores 3, 2, and 1).
model_df['review_score_binary'] = np.where(model_df['review_score'] >= 4, 1, 0)

# Verify the transformation
print(model_df[['review_score', 'review_score_binary']].head())

"""## 6.2 For categorical features"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Initialize the LabelEncoder for categorical features
le_customer_state = LabelEncoder()
le_merged_category = LabelEncoder()

# Apply LabelEncoder to 'customer_state' and 'merged_category'
model_df['customer_state_encoded'] = le_customer_state.fit_transform(model_df['customer_state'])
model_df['merged_category_encoded'] = le_merged_category.fit_transform(model_df['merged_category'])

# generate binary values using get_dummies
# 'prefix' is used at the start of the column names
model_df = pd.get_dummies(model_df, columns=["payment_type"], prefix=["payment_type"], dtype='int', drop_first=True)

# Display the transformed DataFrame
model_df

print(model_df.columns)

"""## 6.3 Select columns for modeling


"""

# Columns to use for modeling
selected_columns = [
    'price',
    'product_description_lenght',
    'product_photos_qty',
    'product_weight_g',
    'product_volume',
    'delivery_duration',
    'delivery_on_time',
    'merged_category_encoded',
    'customer_state_encoded',
    'payment_type_credit_card',
    'payment_type_debit_card',
    'payment_type_voucher',
    'review_score_binary'  # Target variable
]

# Subset the DataFrame
df_model = model_df[selected_columns]

# Check the resulting DataFrame
df_model

print("Checking for NaN values...")
print(df_model.isna().sum())

print("\nChecking for infinite values...")
print(np.isinf(df_model).sum())

"""# 7. Train-Test Split"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Separate features and target
x_values = df_model.drop(columns=['review_score_binary'])
y_value = df_model['review_score_binary']

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(x_values, y_value, test_size=0.2, random_state=42)

# Scale training and test features independently
scaler = MinMaxScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

# Flatten target arrays (if needed by your model)
Y_train = np.ravel(Y_train)
Y_test = np.ravel(Y_test)

# Print the shapes to verify
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

"""# 8. Model

Considering the dataset is imbalanced and the target feature is binary, focusing on time efficiency, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), and Extreme Gradient Boosting (XGB) are applied for their complementary strengths.

## 8.1 Logistic Regression

1) Train the model
"""

from sklearn.linear_model import LogisticRegression as LogR
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
LogR_algo = LogR()
LogR_model = LogR_algo.fit(X_train, Y_train)

# Predict on the train data
predict = LogR_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""2) Hyperparameter tune and see the scores again (on training data)"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

# create a hyperparameter search function for re-usability
def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

LogR_tuned_parameters = {
    'solver': ['liblinear'], # only this one as it does both L1 and L2
    'C': uniform(loc=0.1, scale=19.9),  # Draw from a uniform distribution between 0.1 and 20
    'penalty': ['l1', 'l2', 'elasticnet', None]
}

LogR_best_params = random_search(LogR_algo, LogR_tuned_parameters, X_train, Y_train)

# Train the models
LogR_algo = LogR(**LogR_best_params)
LogR_model = LogR_algo.fit(X_train, Y_train)

# Predict on the train data
predict = LogR_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) The test scores on test data."""

# predict based on TEST data
predict = LogR_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Logistic Regression Confusion Matrix")
predict = LogR_model.predict(X_test)
CM.from_predictions(Y_test, predict)

"""## 8.2 Decision Tree

1) Train the model.
"""

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score

# Initialize models
tree_algo = DTC()
tree_model = tree_algo.fit(X_train, Y_train)

# Predict on the train data
predict = tree_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

print(f'Accuracy: {round(accuracy_score(Y_train, predict),2)}')

"""The Decision Tree model demonstrates exceptionally high performance, which suggests the potential for overfitting. As a next step, hyperparameter tuning will be conducted to improve generalization and mitigate overfitting.

2) Hyperparameter tune and see the scores again (on training data)
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

# create a hyperparameter search function for re-usability
def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

tree_tuned_parameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(3, 9),  # Draw from a uniform distribution between 3 and 15
    'min_samples_split': randint(3, 9),  # Draw from a uniform distribution between 2 and 10
    'max_features': ['sqrt', 'log2', None]
}

tree_best_params = random_search(tree_algo, tree_tuned_parameters, X_train, Y_train)

# Train the models
tree_algo = DTC(**tree_best_params)
tree_model = tree_algo.fit(X_train, Y_train)

# Predict on the train data
predict = tree_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""The adjusted scores now appear balanced and indicate strong performance.

3) Test score on test data.
"""

# predict based on TEST data
predict = tree_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Decision Tree Confusion Matrix")
predict = tree_model.predict(X_test)
CM.from_predictions(Y_test, predict)

"""## 8.3 Random Forest

1) Train the model.
"""

from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
RF_algo = RF()
RF_model = RF_algo.fit(X_train, Y_train)

# Predict on the train data
predict = RF_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""Similar to the Decision Tree, the scores are high but indicate overfitting.

2) Hyperparameter tune and see the scores again (on training data)
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

RF_tuned_parameters = {
    'n_estimators': randint(50, 500), # Draw from a uniform distribution between 50 and 500
    'max_depth': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'min_samples_split': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'max_features': ['sqrt', 'log2']
}

RF_best_params = random_search(RF_algo, RF_tuned_parameters, X_train, Y_train)

# Train the models
RF_algo = RF(**RF_best_params)
RF_model = RF_algo.fit(X_train, Y_train)

# Predict on the train data
predict = RF_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) Test score on test data."""

# predict based on TEST data
predict = RF_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Random Forest Confusion Matrix")
predict = RF_model.predict(X_test)
CM.from_predictions(Y_test, predict)

"""## 8.4 XGB

1) Train the model.
"""

from xgboost import XGBClassifier as XGB
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score

# Initialize models
XGB_algo = XGB()
XGB_model = XGB_algo.fit(X_train, Y_train)

# Predict on the train data
predict = XGB_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""2) Hyperparameter tune and see the scores again (on training data)"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings("ignore")

def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

XGB_tuned_parameters = {
    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500
    # eta is learning rate
    'eta': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5
    # objective is the same as criterion
    'objective': ['binary:logistic', 'binary:hinge'],
    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7
}

XGB_best_params = random_search(XGB_algo, XGB_tuned_parameters, X_train, Y_train)

# Train the models
XGB_algo = XGB(**XGB_best_params)
XGB_model = XGB_algo.fit(X_train, Y_train)

# Predict on the train data
predict = XGB_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) Test score on test data."""

# predict based on TEST data
predict = XGB_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, predict)
print(f"Accuracy: {accuracy}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("XGB Confusion Matrix")
predict = XGB_model.predict(X_test)
CM.from_predictions(Y_test, predict)

"""After applying the four modeling methods, the evaluation metrics indicate high scores for the micro-averaged metrics. The performance metrics on both the training and testing datasets are similar, indicating that the model likely achieves a good balance between bias and variance, with no evidence of overfitting or underfitting.

However, the overall performance is suboptimal. This could be attributed to the imbalanced dataset, which contains a disproportionately large number of instances for the class labeled as 1. As a result, the models struggle to accurately predict the minority class (labeled as 0) and exhibit slight overfitting when predicting the majority class (labeled as 1).

# 9. Opti-SMOTE

As the dataset is imbalanced, SMOTE is applied to generate additional synthetic data for the minority class (class 0). This helps address the class imbalance and improve the model's ability to predict both classes more effectively.
"""

from imblearn.over_sampling import SMOTE

# create an SMOTE instance that will return 2x as many majority as minority class
smote = SMOTE(random_state=42, sampling_strategy=0.5)

X_train, Y_train = smote.fit_resample(X_train, Y_train)

# get the value countes by temporarily converting to a dataframe
pd.Series(Y_train).value_counts()

"""After applying SMOTE to address the class imbalance, the four methods—Logistic Regression, Decision Tree, Random Forest, and Extreme Gradient Boosting—are applied again to evaluate their performance on the balanced dataset. This might helps to assess the impact of SMOTE in improving the models' ability to predict both classes more accurately.

## 9.1 Logistic Regression

1) Train the model
"""

from sklearn.linear_model import LogisticRegression as LogR
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
LogR_algo = LogR()
LogR_model = LogR_algo.fit(X_train, Y_train)

# Predict on the train data
predict = LogR_model.predict(X_train)

from sklearn.metrics import accuracy_score
# Calculate the accuracy
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""2) Hyperparameter tune and see the scores again (on training data)"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

# create a hyperparameter search function for re-usability
def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

LogR_tuned_parameters = {
    'solver': ['liblinear'], # only this one as it does both L1 and L2
    'C': uniform(loc=0.1, scale=19.9),  # Draw from a uniform distribution between 0.1 and 20
    'penalty': ['l1', 'l2', 'elasticnet', None]
}

LogR_best_params = random_search(LogR_algo, LogR_tuned_parameters, X_train, Y_train)

# Train the models
LogR_algo = LogR(**LogR_best_params)
LogR_model = LogR_algo.fit(X_train, Y_train)

# Predict on the train data
predict = LogR_model.predict(X_train)

from sklearn.metrics import accuracy_score
# Calculate the accuracy
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) The test score on test data."""

# predict based on TEST data
LogR_predict = LogR_model.predict(X_test)

from sklearn.metrics import accuracy_score
# Calculate the accuracy
accuracy = accuracy_score(Y_test, LogR_predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, LogR_predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, LogR_predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

# Confusion Matrix
from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Logistic Regression Confusion Matrix")
predict = LogR_model.predict(X_test)
CM.from_predictions(Y_test, LogR_predict)

"""## 9.2 Decision Tree

1) Train the model.
"""

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
tree_algo = DTC()
tree_model = tree_algo.fit(X_train, Y_train)

# Predict on the train data
predict = tree_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""Obviously, it's overfitting.

2) Hyperparameter tune and see the scores again (on training data)
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

tree_tuned_parameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(3, 9),  # Draw from a uniform distribution between 3 and 15
    'min_samples_split': randint(3, 9),  # Draw from a uniform distribution between 2 and 10
    'max_features': ['sqrt', 'log2', None]
}

tree_best_params = random_search(tree_algo, tree_tuned_parameters, X_train, Y_train)

# Train the models
tree_algo = DTC(**tree_best_params)
tree_model = tree_algo.fit(X_train, Y_train)

# Predict on the train data
predict = tree_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) Test score on test data."""

# predict based on TEST data
tree_predict = tree_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, tree_predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, tree_predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, tree_predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Decision Tree Confusion Matrix")
predict = tree_model.predict(X_test)
CM.from_predictions(Y_test, tree_predict)

"""## 9.3 Random Forest

1) Train the model.
"""

from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
RF_algo = RF()
RF_model = RF_algo.fit(X_train, Y_train)

# Predict on the train data
predict = RF_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""Overfitting again.

2) Hyperparameter tune and see the scores again (on training data)
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

RF_tuned_parameters = {
    'n_estimators': randint(50, 500), # Draw from a uniform distribution between 50 and 500
    'max_depth': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'min_samples_split': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'max_features': ['sqrt', 'log2']
}

RF_best_params = random_search(RF_algo, RF_tuned_parameters, X_train, Y_train)

# Train the models
RF_algo = RF(**RF_best_params)
RF_model = RF_algo.fit(X_train, Y_train)

# Predict on the train data
predict = RF_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) Test score on test data."""

# predict based on TEST data
RF_predict = RF_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, RF_predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, RF_predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, RF_predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("Random Forest Confusion Matrix")
predict = RF_model.predict(X_test)
CM.from_predictions(Y_test, predict)

"""## 9.4 XGB

1) Train the model.
"""

from xgboost import XGBClassifier as XGB
from sklearn.metrics import precision_recall_fscore_support

# Initialize models
XGB_algo = XGB()
XGB_model = XGB_algo.fit(X_train, Y_train)

# Predict on the train data
predict = XGB_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""2) Hyperparameter tune and see the scores again (on training data)"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

import warnings
warnings.filterwarnings("ignore")

def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

XGB_tuned_parameters = {
    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500
    # eta is learning rate
    'eta': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5
    # objective is the same as criterion
    'objective': ['binary:logistic', 'binary:hinge'],
    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7
}

XGB_best_params = random_search(XGB_algo, XGB_tuned_parameters, X_train, Y_train)

# Train the models
XGB_algo = XGB(**XGB_best_params)
XGB_model = XGB_algo.fit(X_train, Y_train)

# Predict on the train data
predict = XGB_model.predict(X_train)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_train, predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_train, predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")

"""3) Test score on test data."""

# predict based on TEST data
XGB_predict = XGB_model.predict(X_test)

# Calculate the accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test, XGB_predict)
print(f"Accuracy: {accuracy}")

# Micro-averaged metrics
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y_test, XGB_predict, average='micro')
print(f"Micro Precision: {precision_micro}")
print(f"Micro Recall: {recall_micro}")
print(f"Micro F1-score: {f1_micro}")
print("\n")

# Macro-averaged metrics
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y_test, XGB_predict, average='macro')
print(f"Macro Precision: {precision_macro}")
print(f"Macro Recall: {recall_macro}")
print(f"Macro F1-score: {f1_macro}")
print("\n")

from sklearn.metrics import ConfusionMatrixDisplay as CM
print("XGB Confusion Matrix")
predict = XGB_model.predict(X_test)
CM.from_predictions(Y_test, XGB_predict)

"""# 10. EVALUATION

## 10.1 Comparative Analysis

Compare the performance of different models to determine the most suitable one.
"""

from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

# Define y_true
y_true = Y_test

# List of models and their predictions
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost']
y_preds = [LogR_predict, tree_predict, RF_predict, XGB_predict]

# Initialize dictionaries to store metrics
micro_metrics = {}
macro_metrics = {}
confusion_matrices = {}

# Iterate through models to calculate metrics
for model, y_pred in zip(models, y_preds):
    # Calculate Precision, Recall, F1-Score for Micro and Macro averages
    precision_micro = precision_recall_fscore_support(y_true, y_pred, average='micro')[0]
    recall_micro = precision_recall_fscore_support(y_true, y_pred, average='micro')[1]
    f1_micro = precision_recall_fscore_support(y_true, y_pred, average='micro')[2]

    precision_macro = precision_recall_fscore_support(y_true, y_pred, average='macro')[0]
    recall_macro = precision_recall_fscore_support(y_true, y_pred, average='macro')[1]
    f1_macro = precision_recall_fscore_support(y_true, y_pred, average='macro')[2]

    # Store the results
    micro_metrics[model] = [precision_micro, recall_micro, f1_micro]
    macro_metrics[model] = [precision_macro, recall_macro, f1_macro]

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    confusion_matrices[model] = cm

# Convert results to DataFrames for better presentation
micro_df = pd.DataFrame(micro_metrics, index=['Precision', 'Recall', 'F1-Score'])
macro_df = pd.DataFrame(macro_metrics, index=['Precision', 'Recall', 'F1-Score'])

# Display Results
print("Micro Average Metrics:\n", micro_df)
print("\nMacro Average Metrics:\n", macro_df)

# Display Confusion Matrices
for model in models:
    print(f"\nConfusion Matrix for {model}:\n", confusion_matrices[model])

"""The performance evaluation reveals that while all models exhibit acceptable results, the LR, RF and DT models show signs of underfitting. This conclusion is supported by the observation that their testing data scores are consistently higher than their training data scores. This discrepancy indicates that these models may lack sufficient complexity to capture the underlying patterns in the training dataset effectively, resulting in limited predictive capacity during the training phase.

Considering SMOTE for class 0 data is necessary, we ultimately selected the XGB (XGBoost) model for final modeling and further tuning.

## 10.2 Further Tuning for XGB

Use a weighted score to balance recall and precision during hyperparameter tuning to ensure the model performs well in line with the application's priorities.

By assigning 70% weight to recall and 30% weight to precision which ensures the model captures the majority of good reviews while maintaining acceptable accuracy in the predictions.
"""

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score, precision_recall_fscore_support
from sklearn.metrics import ConfusionMatrixDisplay as CM
from scipy.stats import uniform, randint
import warnings
warnings.filterwarnings("ignore")

# Step 1: Define a custom weighted metric function
def custom_weighted_metric(y_true, y_pred):
    # Calculate Precision and Recall for positive class (class 1)
    precision = precision_score(y_true, y_pred, pos_label=1)
    recall = recall_score(y_true, y_pred, pos_label=1)
    # Assign weights: 70% Recall, 30% Precision (adjustable)
    return 0.7 * recall + 0.3 * precision

# Step 2: Create the hyperparameter search function
def random_search(algo, hyperparameters, X_train, Y_train):
    # Perform RandomizedSearchCV with custom scoring
    clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                             scoring=make_scorer(custom_weighted_metric), n_iter=20, refit=True)
    clf.fit(X_train, Y_train)
    return clf.best_params_

# Step 3: Define hyperparameters for XGBClassifier
XGB_tuned_parameters = {
    'n_estimators': randint(100, 500),  # Number of trees
    'eta': uniform(loc=0.01, scale=0.49),  # Learning rate
    'objective': ['binary:logistic', 'binary:hinge'],  # Objective function
    'max_depth': randint(3, 15)  # Tree depth
}

# Step 4: Perform hyperparameter tuning
XGB_algo = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
XGB_best_params = random_search(XGB_algo, XGB_tuned_parameters, X_train, Y_train)

# Step 5: Train the final model with best parameters
XGB_algo = XGBClassifier(**XGB_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')
XGB_model = XGB_algo.fit(X_train, Y_train)

# Step 6: Evaluate the model on training and testing sets
for dataset, (X, Y, name) in enumerate([(X_train, Y_train, "Training"), (X_test, Y_test, "Testing")]):
    # Predict on the dataset
    predict = XGB_model.predict(X)

    # Accuracy
    accuracy = accuracy_score(Y, predict)
    print(f"{name} Set Accuracy: {accuracy:.2f}")

    # Micro-averaged metrics
    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(Y, predict, average='micro')
    print(f"Micro Precision: {precision_micro:.2f}")
    print(f"Micro Recall: {recall_micro:.2f}")
    print(f"Micro F1-score: {f1_micro:.2f}")
    print("\n")

    # Macro-averaged metrics
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(Y, predict, average='macro')
    print(f"Macro Precision: {precision_macro:.2f}")
    print(f"Macro Recall: {recall_macro:.2f}")
    print(f"Macro F1-score: {f1_macro:.2f}")
    print("\n")

# Step 7: Visualize Confusion Matrix for Testing Set
print("Confusion Matrix (Testing Set):")
CM.from_predictions(Y_test, XGB_model.predict(X_test))

"""The model performers better with balanced micro metrics across both training and testing sets."""